
#搭建一个Kubernetes/K8S集群（kubeadm） 李振良

# 1.简介
    
    1.部署方式1：kubeadm可帮助你快速部署一套k8s集群，这个工具能通过两条命令完成一个k8s集群部署：
        创建一个master节点：：kubeadm init
        将一个node节点加入到当前集群中：：kubeadm join <Master节点的ip和端口>
        kubeadm 现在可以上生产，但是用的人少，不确定是否稳定
    2.部署方式2：二进制部署 【企业常用，需要了解详细原理，修改各种参数配置】,80%企业使用此种部署方式
        企业生产中，建议使用
        
# 2.安装要求
    
    ・一台或多台运行 deb/rpm兼容操作系统的 机器，例如ubuntu或centos
    ・硬件配置：2GB或更多RAM，2个及以上CPU，硬盘30G或更多
    ・集群中所有机器之间网络互通
    ・可以访问外网，需要拉取镜像
    ・禁止swap分区
    
# 3.学习目标
    
    1.所有节点上安装docker 和 kubeadm
    2.部署kubernetes master
    3.部署容器网络插件
    4.部署kubernetes node，将节点加入kubernetes集群
    5.部署Dashboard Web页面，可视化查看kubernetes资源
    
# 4.准备环境
     
    1.单master多node，学习够用
    2.命令：
        关闭防火墙 - 生产环境不推荐
            systemctl stop firewalld
            systemctl disabled firewalld
        关闭selinux - 对文件的权限控制
            sed -i 's/enbforcing/disabled/' /etc/selinux/config
            setenforce 0  设置set
            getenforce    查看get 
        关闭swap - 分区
            swapoff -a      临时去掉分区
            vim /etc/fstab  永久去掉分区
                /dev/mapper/centos_001-swap swap                    swap    defaults        0 0 【删除此行】
        添加主机名与ip对应关系
            cat /etc/hosts
                192.168.56.100 k8s-master
                192.168.56.110 k8s-node1
                192.168.56.120 k8s-node2
        将桥接的IPV4流量传递到iptables的链
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF 【命令无法执行时，当我们不小心在第二个EOF前多一个空格的话，系统不会把第二个EOF当作结束分界符，而当做标准输入的内容来处理，这个要注意。】
              
              sysctl --system

# 5.所有节点安装docker/kubeadm/kubelet ，准备了三台虚拟机 100,110,120
    
    xshell可以同时操作多个终端：工具-发送键输入到所有对话
    
## 5.1 安装docker
    
    wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo
    yum -y install docker-ce-18.06.1.ce-3.el7 【会有兼容问题，安装指定版本，按官方要求安装docker版本】
    systemctl enable docker && systemctl start docker
    docker --version

## 5.2 添加阿里云yum软件源
    
cat <<EOF > /etc/yum.repos.d/kubernetes.repo 
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg 
EOF  

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

【命令无法执行时，当我们不小心在第二个EOF前多一个空格的话，系统不会把第二个EOF当作结束分界符，而当做标准输入的内容来处理，这个要注意。】
    
    sysctl --system
    yum list kubelet --showduplicates

## 5.3 安装kubeadm，kubelet，kubectl [由于版本更新频繁，此处指定版本号部署]

    yum install -y kubelet-1.13.3 kubeadm-1.13.3 kubectl-1.13.3 【一直在报错，所以修改为1.14.3】
    yum install -y kubelet-1.14.3 kubeadm-1.14.3 kubectl-1.14.3 【最后使用的版本】
    systemctl enable kubelet
    
## 5.4 部署kubernetes Master

kubeadm init \
--apiserver-advertise-address=192.168.56.100 \
--image-repository registry.aliyuncs.com/google_containers \
--kubernetes-version v1.14.3 \
--service-cidr=10.1.0.0/16 \
--pod-network-cidr=10.244.0.0/16

--apiserver-advertise-address=192.168.56.100  指监听的内网地址
--image-repository registry.aliyuncs.com/google_containers 指定镜像仓库
--kubernetes-version v1.14.3 \  指定kubernete版本
--service-cidr=10.1.0.0/16 \    指定service网络的ip地址段，负载均衡的虚拟ip
--pod-network-cidr=10.244.0.0/16 pod网络指的容器使用的IP地址，分配到每个pod
此命令会下载多个镜像，下载完成后可以使用 docker ps 进行查看（耗时长）

    [root@001 tmp]# kubeadm init --apiserver-advertise-address=192.168.56.100 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.14.3 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.244.0.0/16
    [init] Using Kubernetes version: v1.14.3  【初始化】
    [preflight] Running pre-flight checks     【准备工作，检查当前环境是否适合kubernetes】
    	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
    	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
    [preflight] Pulling images required for setting up a Kubernetes cluster 【下载镜像】
    [preflight] This might take a minute or two, depending on the speed of your internet connection
    [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' 【准备工作，检查当前环境是否适合kubernetes】
    [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env" 【生成环境变量文件】
    [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"【生成kubernete配置文件】
    [kubelet-start] Activating the kubelet service
    [certs] Using certificateDir folder "/etc/kubernetes/pki"   【生成证书，存放在该文件夹下面】
    [certs] Generating "etcd/ca" certificate and key
    [certs] Generating "etcd/peer" certificate and key
    [certs] etcd/peer serving cert is signed for DNS names [001.com localhost] and IPs [192.168.56.100 127.0.0.1 ::1]
    [certs] Generating "etcd/server" certificate and key
    [certs] etcd/server serving cert is signed for DNS names [001.com localhost] and IPs [192.168.56.100 127.0.0.1 ::1]
    [certs] Generating "apiserver-etcd-client" certificate and key
    [certs] Generating "etcd/healthcheck-client" certificate and key
    [certs] Generating "ca" certificate and key
    [certs] Generating "apiserver" certificate and key
    [certs] apiserver serving cert is signed for DNS names [001.com kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.1.0.1 192.168.56.100]
    [certs] Generating "apiserver-kubelet-client" certificate and key
    [certs] Generating "front-proxy-ca" certificate and key
    [certs] Generating "front-proxy-client" certificate and key
    [certs] Generating "sa" key and public key
    [kubeconfig] Using kubeconfig folder "/etc/kubernetes" 【核心配置，配置单元组件，怎么访问集群】
    [kubeconfig] Writing "admin.conf" kubeconfig file
    [kubeconfig] Writing "kubelet.conf" kubeconfig file
    [kubeconfig] Writing "controller-manager.conf" kubeconfig file
    [kubeconfig] Writing "scheduler.conf" kubeconfig file
    [control-plane] Using manifest folder "/etc/kubernetes/manifests"【以下pod启动所需要的yaml文件】
    [control-plane] Creating static Pod manifest for "kube-apiserver"【 static Pod静态pod与kubernetes绑定，生命周期同步进行】
    [control-plane] Creating static Pod manifest for "kube-controller-manager"
    [control-plane] Creating static Pod manifest for "kube-scheduler"
    [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
    [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
    [kubelet-check] Initial timeout of 40s passed.
    [apiclient] All control plane components are healthy after 56.003272 seconds
    [upload-config] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace 【将kubeadm的配置文件保存到configMap存储，node加入集群所需的配置】
    [kubelet] Creating a ConfigMap "kubelet-config-1.14" in namespace kube-system with the configuration for the kubelets in the cluster
    [upload-certs] Skipping phase. Please see --experimental-upload-certs
    [mark-control-plane] Marking the node 001.com as control-plane by adding the label "node-role.kubernetes.io/master=''"
    [mark-control-plane] Marking the node 001.com as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
    [bootstrap-token] Using token: kdp0gh.o4svkr15lpzkylww 【用于node申请进入集群时自动颁发证书的】
    [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
    [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
    [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
    [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
    [bootstrap-token] creating the "cluster-info" ConfigMap in the "kube-public" namespace
    [addons] Applied essential addon: CoreDNS   【部署两个必要的插件，k8s内部提供dns解析】
    [addons] Applied essential addon: kube-proxy   【部署两个必要的插件，容器直接提供服务发现的】
    
    Your Kubernetes control-plane has initialized successfully! 【kubernetes初始化完成】
    
    To start using your cluster, you need to run the following as a regular user: 【复制执行以下命令】
    
      mkdir -p $HOME/.kube
      sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
      sudo chown $(id -u):$(id -g) $HOME/.kube/config
    
    You should now deploy a pod network to the cluster.
    Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
      https://kubernetes.io/docs/concepts/cluster-administration/addons/
    
    Then you can join any number of worker nodes by running the following on each as root:
    
    kubeadm join 192.168.56.100:6443 --token kdp0gh.o4svkr15lpzkylww \   【当出现这个的时候表示已经成功】
        --discovery-token-ca-cert-hash sha256:e5bcfc316ace939e71e3f20c05f2cc84d200f74c769debdeac21ec90cdfbba3f  

使用kubectl工具
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):(id -g) $HOME/.kube/config

kubectl get nodes  【执行此命令就可以看见已经有一个节点了】
    NAME      STATUS     ROLES    AGE   VERSION
    001.com   NotReady   master   21m   v1.14.3  【NotReady-未准备好】

## 5.6 安装pod网络插件-确保能够访问quay.io这个registry
    
    kubectl get pod -n kube-system -o wide   
        [root@001 tmp]# kubectl get pod -n kube-system -o wide
        NAME                              READY   STATUS    RESTARTS   AGE   IP               NODE      NOMINATED NODE   READINESS GATES
        coredns-8686dcc4fd-kwtwm          0/1     Pending【未准备就绪】   0          32m   <none>           <none>    <none>           <none>
        coredns-8686dcc4fd-xtzl8          0/1     Pending【未准备就绪】   0          32m   <none>           <none>    <none>           <none>
        etcd-001.com                      1/1     Running   0          32m   192.168.56.100   001.com   <none>           <none>
        kube-apiserver-001.com            1/1     Running   0          32m   192.168.56.100   001.com   <none>           <none>
        kube-controller-manager-001.com   1/1     Running   0          32m   192.168.56.100   001.com   <none>           <none>
        kube-proxy-pz6wt                  1/1     Running   0          32m   192.168.56.100   001.com   <none>           <none>
        kube-scheduler-001.com            1/1     Running   0          32m   192.168.56.100   001.com   <none>           <none>
    kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml 
        可能访问不了
        或者 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
        解决GitHub的raw.githubusercontent.com无法连接问题，打开翻墙工具 【赛风】
        sudo vi /etc/hosts
            # GitHub Start
            52.74.223.119 github.com
            192.30.253.119 gist.github.com
            54.169.195.247 api.github.com
            185.199.111.153 assets-cdn.github.com
            151.101.76.133 raw.githubusercontent.com
            151.101.108.133 user-images.githubusercontent.com
            151.101.76.133 gist.githubusercontent.com
            151.101.76.133 cloud.githubusercontent.com
            151.101.76.133 camo.githubusercontent.com
            151.101.76.133 avatars0.githubusercontent.com
            151.101.76.133 avatars1.githubusercontent.com
            151.101.76.133 avatars2.githubusercontent.com
            151.101.76.133 avatars3.githubusercontent.com
            151.101.76.133 avatars4.githubusercontent.com
            151.101.76.133 avatars5.githubusercontent.com
            151.101.76.133 avatars6.githubusercontent.com
            151.101.76.133 avatars7.githubusercontent.com
            151.101.76.133 avatars8.githubusercontent.com
            # GitHub End
    可以下载下来查看内容：：wget https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml 
    kubectl get pod -n kube-system -o wide 还是报错==========================================
